{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-14.1-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset_builder, load_dataset, concatenate_datasets\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import torch\n",
    "import platform\n",
    "import evaluate\n",
    "\n",
    "print(platform.platform())\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../Data')\n",
    "\n",
    "import time\n",
    "from ratelimiter import RateLimiter\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_dataset = load_dataset('samsum')\n",
    "dialogue_dataset = concatenate_datasets([dialogue_dataset[k] for k in dialogue_dataset.keys()])\n",
    "dialogue_dataset = dialogue_dataset.rename_column('dialogue', 'document')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratelimiter import RateLimiter\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMALITY globals\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/s-nlp/roberta-base-formality-ranker\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion intensity globals\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "new_col_names = {\n",
    "    'article':'document',\n",
    "    'highlights':'summary',\n",
    "    'documents':'document',\n",
    "    'tldr':'summary',\n",
    "    'dialogue':'document'\n",
    "}\n",
    "\n",
    "keep_cols = ['document', 'summary', 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxicity globals\n",
    "\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "API_KEY = 'AIzaSyDcA-LYHVNateEydAvPLg5AaF19sZwM-mY'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formality_query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def read_lexicon(lex_path):\n",
    "    lex_df = pd.read_csv(lex_path, sep='\\t')\n",
    "    lex_df.columns = ['word', 'intensity_score']\n",
    "    lex_df.drop_duplicates(subset=['word'], inplace=True)\n",
    "    lex_df.set_index('word', inplace=True)\n",
    "    return lex_df\n",
    "\n",
    "\n",
    "def read_csv_dataset(text_path):\n",
    "    text_df = pd.read_csv(text_path, usecols=['summary'])\n",
    "    text_df['summary'] = text_df['summary'].apply(lambda sentence: re.findall(r'\\w+', sentence.lower()))\n",
    "    return text_df\n",
    "\n",
    "\n",
    "def read_huggingfce_dataset(model_strs):\n",
    "    data = load_dataset(*model_strs)\n",
    "    text_df = pd.DataFrame()\n",
    "    for k in data.keys():\n",
    "        text_df = pd.concat([text_df, data[k].to_pandas()])\n",
    "    text_df = text_df.rename(columns=new_col_names)\n",
    "    text_df['summary'] = text_df['summary'].apply(lambda sentence: re.findall(r'\\w+', sentence.lower()))\n",
    "    if 'id' in text_df.columns:\n",
    "        return text_df[['id', 'document', 'summary']]\n",
    "    return text_df[['document', 'summary']]\n",
    "\n",
    "\n",
    "def toxicity_query(text):\n",
    "    analyze_request = {\n",
    "        'comment': { 'text': text },\n",
    "        'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Measurement:\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_df,\n",
    "            dataset_specs,\n",
    "            rate_limiter_params={'max_calls':64, 'period':10},\n",
    "            no_samples=1000,\n",
    "            lex_dir_prefix = '/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/Data/NRC-Emotion-Intensity-Lexicon/OneFilePerEmotion/',\n",
    "            DEBUG=False\n",
    "        ):\n",
    "        self.data_df = data_df\n",
    "        self.rate_limiter = RateLimiter(**rate_limiter_params)\n",
    "        assert(Path(lex_dir_prefix).exists())\n",
    "        self.lex_dir_prefix = lex_dir_prefix\n",
    "        \n",
    "        assert(k in dataset_specs.keys() for k in ['generation', 'subject']), \"Must supply the dataset specs\"\n",
    "        self.config = {\n",
    "            'subject': dataset_specs['subject'],\n",
    "            'generation': dataset_specs['generation'],\n",
    "            'no_samples': no_samples,\n",
    "            'DEBUG': DEBUG\n",
    "        }\n",
    "        \n",
    "    def compute_coverage(self):\n",
    "        coverage = self.data_df.apply(lambda x: len(set(x['summary_toks']).intersection(set(x['document_toks']))), axis=1)\n",
    "        coverage = coverage.divide(self.data_df['document_toks'].apply(lambda x: len(set(x))))\n",
    "        return coverage.mean()\n",
    "    \n",
    "    def compute_compression_ratio(self):\n",
    "        ratio = self.data_df['summary'].apply(lambda x: len(x.split(\" \"))).divide(self.data_df['document'].apply(lambda x: len(x.split(\" \"))))\n",
    "        return ratio.mean()\n",
    "    \n",
    "    def compute_summary_token_distribution(self):\n",
    "        fdist = FreqDist(tok for tok in list(chain.from_iterable(self.data_df['summary_toks'])))\n",
    "        return fdist\n",
    "    \n",
    "    \n",
    "    def evaluate_formality(self):\n",
    "        sample_idxs = rng.integers(0, self.data_df.shape[0], size=self.config['no_samples'])\n",
    "        formality_scores = np.zeros(self.config['no_samples'])\n",
    "        print(\"Evaluating formality ...\")\n",
    "        for idx in tqdm(range(self.config['no_samples'])):\n",
    "            with self.rate_limiter:\n",
    "                response = formality_query({\n",
    "                    \"inputs\": self.data_df.iloc[sample_idxs[idx]]['summary']\n",
    "                })\n",
    "                try:\n",
    "                    formality_scores[idx] = response[0][0]['score']\n",
    "                except:\n",
    "                    assert('error' in response.keys())\n",
    "                    print(f\"Formality Eval - Time limit exceeded, sleeping for 10sec, No. samples evaluated = {idx}\")\n",
    "                    time.sleep(10)\n",
    "                    idx -= 1\n",
    "        if self.config['DEBUG']:\n",
    "            return formality_scores, sample_idxs\n",
    "        return formality_scores\n",
    "    \n",
    "    \n",
    "    def evaluate_toxicity(self):\n",
    "        sample_idxs = rng.integers(0, self.data_df.shape[0], size=self.config['no_samples']).astype(int)\n",
    "        toxicity_scores = np.zeros(self.config['no_samples'])\n",
    "        print(\"Evaluating toxicity\")\n",
    "        for idx in tqdm(range(self.config['no_samples'])):\n",
    "            with self.rate_limiter:\n",
    "                try:\n",
    "                    response = toxicity_query(self.data_df.iloc[int(sample_idxs[idx])]['summary'])\n",
    "                    toxicity_scores[idx] = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "                except HttpError:\n",
    "                    print(f\"Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = {idx}\")\n",
    "                    time.sleep(69)\n",
    "                    idx -= 1\n",
    "        if self.config['DEBUG']:\n",
    "            return toxicity_scores, sample_idxs\n",
    "        return toxicity_scores\n",
    "    \n",
    "    \n",
    "    def evaluate_emotion_intensity(\n",
    "            self,\n",
    "            lex_dir_suffix = '-NRC-Emotion-Intensity-Lexicon-v1.txt', \n",
    "            lex_names = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\n",
    "        ):\n",
    "        \"\"\"\n",
    "        What it do: \n",
    "            Create a dataframe where each column corresponds to one of the 8 emotions, \n",
    "            the value corresponds to the sum of intensity scores divided by the \n",
    "            number of tokens in the summary.\n",
    "        \"\"\"\n",
    "        emot_scores_df = pd.DataFrame()\n",
    "        summ_tok_count = self.data_df['summary'].apply(lambda x: len(x)).to_numpy()\n",
    "        weighted_avg = np.zeros(self.data_df.shape[0])\n",
    "        \n",
    "        print(\"Evaluating emotion intensity ...\")\n",
    "        for LEX in tqdm(lex_names):\n",
    "            lex_path = self.lex_dir_prefix + LEX + lex_dir_suffix\n",
    "            lex_df = read_lexicon(lex_path)\n",
    "            score_var, cnt_var = f\"{LEX}_score_avg\", f\"{LEX}_tok_cnt\"\n",
    "            res = self.data_df['summary'].apply(lambda toks: lex_df.index.str.fullmatch('|'.join(toks)))\n",
    "            emot_scores_df[score_var] = res.apply(lambda emot_toks: lex_df[emot_toks]['intensity_score'].mean()).fillna(0)\n",
    "            emot_scores_df[cnt_var] = np.stack(res.values, dtype=int).sum(axis=1)\n",
    "            w = np.divide(emot_scores_df[cnt_var].to_numpy(), summ_tok_count) * emot_scores_df[score_var].to_numpy()\n",
    "            weighted_avg = np.add(weighted_avg, w)\n",
    "            \n",
    "        emot_scores_df = emot_scores_df.fillna(0)\n",
    "        emot_scores_df['num_summary_tokens'] = summ_tok_count\n",
    "        emot_scores_df[\"weighted_avg\"] = weighted_avg\n",
    "            \n",
    "        return emot_scores_df.iloc[:,::-1]\n",
    "        \n",
    "        \n",
    "    def measure(self):\n",
    "        measurements = {}\n",
    "        \n",
    "        self.data_df['document_toks'] = self.data_df['document'].apply(lambda sentence: re.findall(r'\\w+', sentence.lower()))\n",
    "        self.data_df['summary_toks'] = self.data_df['summary'].apply(lambda sentence: re.findall(r'\\w+', sentence.lower()))\n",
    "        \n",
    "        measurements['coverage'] = self.compute_coverage()\n",
    "        measurements['compression_ratio'] = self.compute_compression_ratio()\n",
    "        measurements['summary_token_distribution'] = self.compute_summary_token_distribution()\n",
    "        \n",
    "        formality_eval = self.evaluate_formality()\n",
    "        toxicity_eval = self.evaluate_toxicity()\n",
    "        \n",
    "        if self.config['DEBUG']:\n",
    "            measurements['formality_scores'], measurements['formality_sample_idxs'] = formality_eval\n",
    "            measurements['toxicity_scores'], measurements['toxicity_sample_idxs'] = toxicity_eval\n",
    "        else:\n",
    "            measurements['formality_scores'] = formality_eval\n",
    "            measurements['toxicity_scores'] = toxicity_eval\n",
    "        measurements['formality_mean'] = measurements['formality_scores'].mean()\n",
    "        measurements['toxicity_mean'] = measurements['toxicity_scores'].mean()\n",
    "        \n",
    "        emot_df = self.evaluate_emotion_intensity()\n",
    "        measurements['emotion_intensity_mean'] = emot_df['weighted_avg'].to_numpy().mean()\n",
    "        measurements['emotion_intensity_measurements'] = emot_df.to_dict()\n",
    "        \n",
    "        return self.config, measurements\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating formality ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:07<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating toxicity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 74/1000 [00:12<02:30,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 76/1000 [01:21<3:45:24, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 77/1000 [02:30<7:56:13, 30.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 81/1000 [03:39<3:44:28, 14.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 82/1000 [04:48<7:53:51, 30.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 92/1000 [05:59<32:16,  2.13s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 103/1000 [07:09<17:04,  1.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 175/1000 [08:29<02:44,  5.03it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 176/1000 [09:38<4:13:06, 18.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 179/1000 [10:47<3:47:23, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 182/1000 [11:56<3:57:49, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 183/1000 [13:05<6:49:29, 30.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 185/1000 [14:15<6:38:49, 29.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 186/1000 [15:24<9:08:14, 40.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 187/1000 [16:33<10:57:59, 48.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 195/1000 [17:42<1:42:44,  7.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 328/1000 [19:12<03:33,  3.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 336/1000 [20:22<27:09,  2.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 339/1000 [21:31<1:47:59,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 341/1000 [22:41<3:42:35, 20.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 342/1000 [23:50<5:46:20, 31.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 360/1000 [25:01<04:33,  2.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 430/1000 [26:20<03:14,  2.94it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 431/1000 [27:30<2:53:34, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 432/1000 [28:39<5:03:57, 32.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 433/1000 [29:48<6:41:07, 42.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 444/1000 [30:58<22:56,  2.48s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 446/1000 [32:07<2:10:31, 14.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 511/1000 [33:26<06:34,  1.24it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 552/1000 [34:40<00:51,  8.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 624/1000 [36:00<01:23,  4.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 625/1000 [37:09<1:44:30, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 631/1000 [38:19<53:07,  8.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 636/1000 [39:28<47:00,  7.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 641/1000 [40:38<48:57,  8.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 669/1000 [41:50<00:41,  8.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 735/1000 [43:09<02:16,  1.94it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 743/1000 [44:19<12:58,  3.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 747/1000 [45:28<30:17,  7.19s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 846/1000 [46:51<00:19,  7.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 847/1000 [48:00<49:15, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 850/1000 [49:10<42:15, 16.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 857/1000 [50:19<15:34,  6.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 865/1000 [51:29<07:55,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 870/1000 [52:39<13:08,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 942/1000 [53:59<00:11,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 943/1000 [55:08<16:47, 17.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 945/1000 [56:17<20:51, 22.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 947/1000 [57:26<22:35, 25.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 948/1000 [58:35<33:07, 38.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 951/1000 [59:45<19:11, 23.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 959/1000 [1:00:55<03:01,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Eval - Time limit exceeded, sleeping for 69sec, No. samples evaluated = 959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:02:08<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating emotion intensity ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "nothing to repeat at position 66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset_specs \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mgeneration\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m0\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m'\u001b[39m:subject\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m measurements \u001b[39m=\u001b[39m Measurement(dialogue_df, dataset_specs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m config, measured2 \u001b[39m=\u001b[39m measurements\u001b[39m.\u001b[39;49mmeasure()\n",
      "\u001b[1;32m/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m measurements[\u001b[39m'\u001b[39m\u001b[39mformality_mean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m measurements[\u001b[39m'\u001b[39m\u001b[39mformality_scores\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m measurements[\u001b[39m'\u001b[39m\u001b[39mtoxicity_mean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m measurements[\u001b[39m'\u001b[39m\u001b[39mtoxicity_scores\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m emot_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_emotion_intensity()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m measurements[\u001b[39m'\u001b[39m\u001b[39memotion_intensity_mean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m emot_df[\u001b[39m'\u001b[39m\u001b[39mweighted_avg\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m measurements[\u001b[39m'\u001b[39m\u001b[39memotion_intensity_measurements\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m emot_df\u001b[39m.\u001b[39mto_dict()\n",
      "\u001b[1;32m/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m lex_df \u001b[39m=\u001b[39m read_lexicon(lex_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m score_var, cnt_var \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mLEX\u001b[39m}\u001b[39;00m\u001b[39m_score_avg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mLEX\u001b[39m}\u001b[39;00m\u001b[39m_tok_cnt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_df[\u001b[39m'\u001b[39;49m\u001b[39msummary\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m toks: lex_df\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mfullmatch(\u001b[39m'\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(toks)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m emot_scores_df[score_var] \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m emot_toks: lex_df[emot_toks][\u001b[39m'\u001b[39m\u001b[39mintensity_score\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean())\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m emot_scores_df[cnt_var] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(res\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/pandas/core/series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4753\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4754\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   4755\u001b[0m         func,\n\u001b[1;32m   4756\u001b[0m         convert_dtype\u001b[39m=\u001b[39;49mconvert_dtype,\n\u001b[1;32m   4757\u001b[0m         by_row\u001b[39m=\u001b[39;49mby_row,\n\u001b[1;32m   4758\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   4759\u001b[0m         kwargs\u001b[39m=\u001b[39;49mkwargs,\n\u001b[0;32m-> 4760\u001b[0m     )\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[39m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[39m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39mdtype, CategoricalDtype) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_map_values(\n\u001b[1;32m   1288\u001b[0m     mapper\u001b[39m=\u001b[39;49mcurried, na_action\u001b[39m=\u001b[39;49maction, convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype\n\u001b[1;32m   1289\u001b[0m )\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2920\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m lex_df \u001b[39m=\u001b[39m read_lexicon(lex_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m score_var, cnt_var \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mLEX\u001b[39m}\u001b[39;00m\u001b[39m_score_avg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mLEX\u001b[39m}\u001b[39;00m\u001b[39m_tok_cnt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_df[\u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m toks: lex_df\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mfullmatch(\u001b[39m'\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(toks)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m emot_scores_df[score_var] \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m emot_toks: lex_df[emot_toks][\u001b[39m'\u001b[39m\u001b[39mintensity_score\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean())\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/src/Measurement/measurements_notebook.ipynb#X33sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m emot_scores_df[cnt_var] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(res\u001b[39m.\u001b[39mvalues, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/pandas/core/strings/accessor.py:136\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    132\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot use .str.\u001b[39m\u001b[39m{\u001b[39;00mfunc_name\u001b[39m}\u001b[39;00m\u001b[39m with values of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minferred dtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    135\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/pandas/core/strings/accessor.py:1391\u001b[0m, in \u001b[0;36mStringMethods.fullmatch\u001b[0;34m(self, pat, case, flags, na)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[39m@forbid_nonstring_types\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   1355\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfullmatch\u001b[39m(\u001b[39mself\u001b[39m, pat, case: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, flags: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, na\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1356\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m \u001b[39m    Determine if each string entirely matches a regular expression.\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[39m    dtype: bool\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1391\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39;49m_str_fullmatch(pat, case\u001b[39m=\u001b[39;49mcase, flags\u001b[39m=\u001b[39;49mflags, na\u001b[39m=\u001b[39;49mna)\n\u001b[1;32m   1392\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_result(result, fill_value\u001b[39m=\u001b[39mna, returns_string\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/pandas/core/strings/object_array.py:234\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_fullmatch\u001b[0;34m(self, pat, case, flags, na)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m case:\n\u001b[1;32m    232\u001b[0m     flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mIGNORECASE\n\u001b[0;32m--> 234\u001b[0m regex \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49mcompile(pat, flags\u001b[39m=\u001b[39;49mflags)\n\u001b[1;32m    236\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: regex\u001b[39m.\u001b[39mfullmatch(x) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_str_map(f, na_value\u001b[39m=\u001b[39mna, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdtype(\u001b[39mbool\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/re.py:251\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile\u001b[39m(pattern, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/re.py:303\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sre_compile\u001b[39m.\u001b[39misstring(pattern):\n\u001b[1;32m    302\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfirst argument must be string or compiled pattern\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 303\u001b[0m p \u001b[39m=\u001b[39m sre_compile\u001b[39m.\u001b[39;49mcompile(pattern, flags)\n\u001b[1;32m    304\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_cache) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    306\u001b[0m         \u001b[39m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/sre_compile.py:788\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[1;32m    787\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[0;32m--> 788\u001b[0m     p \u001b[39m=\u001b[39m sre_parse\u001b[39m.\u001b[39;49mparse(p, flags)\n\u001b[1;32m    789\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    790\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/sre_parse.py:955\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    952\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m    954\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39;49m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    956\u001b[0m \u001b[39mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    957\u001b[0m     \u001b[39m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[39m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[1;32m    959\u001b[0m     state \u001b[39m=\u001b[39m State()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/sre_parse.py:444\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    442\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[1;32m    443\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    447\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/sre_parse.py:669\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    667\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m item \u001b[39mor\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m AT:\n\u001b[0;32m--> 669\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mnothing to repeat\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    670\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m here \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(this))\n\u001b[1;32m    671\u001b[0m \u001b[39mif\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m _REPEATCODES:\n\u001b[1;32m    672\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mmultiple repeat\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    673\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m here \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(this))\n",
      "\u001b[0;31merror\u001b[0m: nothing to repeat at position 66"
     ]
    }
   ],
   "source": [
    "dialogue_df = dialogue_dataset.to_pandas()\n",
    "subject = 'dialogue'\n",
    "dataset_specs = {\n",
    "        'generation':0, \n",
    "        'subject':subject\n",
    "    }\n",
    "\n",
    "measurements = Measurement(dialogue_df, dataset_specs)\n",
    "\n",
    "config, measured2 = measurements.measure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_df = dialogue_dataset.to_pandas()\n",
    "subject = 'dialogue'\n",
    "dataset_specs = {\n",
    "        'generation':0, \n",
    "        'subject':subject\n",
    "    }\n",
    "\n",
    "measurements = Measurement(dialogue_df, dataset_specs)\n",
    "\n",
    "config, measured2 = measurements.measure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['coverage', 'compression_ratio', 'summary_token_distribution', 'formality_scores', 'toxicity_scores', 'formality_mean', 'toxicity_mean'])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measured2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.600000\n",
      "1        0.687500\n",
      "2        0.157303\n",
      "3        0.826087\n",
      "4        0.169811\n",
      "           ...   \n",
      "16364    0.084848\n",
      "16365    0.195876\n",
      "16366    0.067873\n",
      "16367    0.500000\n",
      "16368    0.256098\n",
      "Length: 16369, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dialogue_df = dialogue_dataset.to_pandas()\n",
    "\n",
    "measurements = Measurement(dialogue_df)\n",
    "\n",
    "df, measured = measurements.measure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = dialogue_df.iloc[:50]\n",
    "subject = 'dialogue'\n",
    "dataset_specs = {\n",
    "        'generation':0, \n",
    "        'subject':subject\n",
    "    }\n",
    "\n",
    "measurements_small = Measurement(small_df, dataset_specs, no_samples=15, DEBUG=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
