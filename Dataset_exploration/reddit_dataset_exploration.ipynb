{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-14.1-arm64-arm-64bit\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/madisonthantu/Desktop/COMS_6998/Final_Project/recursive_LLMs/Dataset_exploration/reddit_dataset_exploration.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS_6998/Final_Project/recursive_LLMs/Dataset_exploration/reddit_dataset_exploration.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS_6998/Final_Project/recursive_LLMs/Dataset_exploration/reddit_dataset_exploration.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m\"\u001b[39m\u001b[39m/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS_6998/Final_Project/recursive_LLMs/Dataset_exploration/reddit_dataset_exploration.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mMeasurement\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mglobals\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS_6998/Final_Project/recursive_LLMs/Dataset_exploration/reddit_dataset_exploration.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mMeasurement\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmeasurement\u001b[39;00m \u001b[39mimport\u001b[39;00m Measurement\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS_6998/Final_Project/recursive_LLMs/Dataset_exploration/reddit_dataset_exploration.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# sys.path.append(os.path.abspath(\"/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/src\"))\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import torch\n",
    "import platform\n",
    "import evaluate\n",
    "\n",
    "print(platform.platform())\n",
    "\n",
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs\"))\n",
    "from src.Measurement import globals\n",
    "from src.Measurement.measurement import Measurement\n",
    "# sys.path.append(os.path.abspath(\"/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/src\"))\n",
    "from . import utils\n",
    "globals.init()\n",
    "print(globals.API_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/madisonthantu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REFs:**\n",
    "- https://huggingface.co/docs/transformers/tasks/summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset_name = 'reddit'\n",
    "model = 'T5'\n",
    "generation = 'Gen0'\n",
    "output_dir=os.path.join(dataset_name, model, generation)\n",
    "output_dir\n",
    "\n",
    "use_small_dataset = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'],\n",
       "    num_rows: 211\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tifu = load_dataset(\"reddit_tifu\", 'long')['train']\n",
    "tifu_small = tifu.shard(num_shards=200, index=0)\n",
    "tifu_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tifu = tifu.add_column\n",
    "# doc_sent_len = [len(re.findall(r'\\w+', sentence)) for sentence in tifu['documents']]\n",
    "# summ_sent_len = [len(re.findall(r'\\w+', sentence)) for sentence in tifu['tldr']]\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, tight_layout=True)\n",
    "\n",
    "# axs[0].hist(doc_sent_len, bins=max(doc_sent_len));\n",
    "# axs[0].set_xlabel('Number of words')\n",
    "# axs[0].set_ylabel('Number of documents')\n",
    "# axs[1].hist(summ_sent_len, bins=max(summ_sent_len));\n",
    "# axs[1].set_xlabel('Number of words')\n",
    "# axs[1].set_ylabel('Number of summaries');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used small dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'],\n",
       "        num_rows: 168\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title'],\n",
       "        num_rows: 43\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_small_dataset:\n",
    "    print(\"Used small dataset\")\n",
    "    dataset = tifu_small.train_test_split(test_size=0.2)\n",
    "else:\n",
    "    dataset = tifu.train_test_split(test_size=0.2)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(dataset, num_samples=3, seed=42):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        title, doc, summ = example['title'], example['documents'], example['tldr']\n",
    "        print(f\">> title: {title}\")\n",
    "        print(\">> documents: {0}...\".format(doc[:doc.find('\\n')]))\n",
    "        print(f\">> summary: {summ}\\n\")\n",
    "\n",
    "# show_samples(dataset, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t5-small': 512, 't5-base': 512, 't5-large': 512, 't5-3b': 512, 't5-11b': 512}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab()\n",
    "# # tokenizer(\"Hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"documents\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"tldr\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f787856fa244c968c319807f5de6f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/168 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2281061ec1a4ad09fcb42ba7aa191f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 168\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ups', 'num_comments', 'upvote_ratio', 'score', 'documents', 'tldr', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 43\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_tifu = dataset.map(preprocess_function, batched=True)\n",
    "print(tokenized_tifu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_tifu['train']['tldr', 'documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bert_score = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge_res = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    rouge_res[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    results = {k: round(v, 4) for k, v in rouge_res.items()}\n",
    "    \n",
    "    bert_res = bert_score.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    results.update(bert_res)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_tifu[\"train\"],\n",
    "    eval_dataset=tokenized_tifu[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514c6ca005194387aa0097fef76164f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.25898814201355,\n",
       " 'eval_rouge1': 0.1199,\n",
       " 'eval_rouge2': 0.0094,\n",
       " 'eval_rougeL': 0.0975,\n",
       " 'eval_rougeLsum': 0.0976,\n",
       " 'eval_gen_len': 19.0,\n",
       " 'eval_precision': [0.8698806166648865,\n",
       "  0.8433854579925537,\n",
       "  0.894987940788269,\n",
       "  0.8902104496955872,\n",
       "  0.822005033493042,\n",
       "  0.8346426486968994,\n",
       "  0.8586152791976929,\n",
       "  0.854336678981781,\n",
       "  0.8422900438308716,\n",
       "  0.8470631837844849,\n",
       "  0.847069263458252,\n",
       "  0.8471750617027283,\n",
       "  0.8341564536094666,\n",
       "  0.8202642202377319,\n",
       "  0.8415041565895081,\n",
       "  0.8850875496864319,\n",
       "  0.8391352891921997,\n",
       "  0.8825553059577942,\n",
       "  0.8332506418228149,\n",
       "  0.842179000377655,\n",
       "  0.8482860326766968,\n",
       "  0.8265595436096191,\n",
       "  0.8666667938232422,\n",
       "  0.864119291305542,\n",
       "  0.8496800661087036,\n",
       "  0.8736864924430847,\n",
       "  0.8241649866104126,\n",
       "  0.8385031223297119,\n",
       "  0.8453615307807922,\n",
       "  0.722138524055481,\n",
       "  0.8670085668563843,\n",
       "  0.8354783058166504,\n",
       "  0.8648706674575806,\n",
       "  0.8375243544578552,\n",
       "  0.8508659601211548,\n",
       "  0.8322177529335022,\n",
       "  0.8338196873664856,\n",
       "  0.8388394117355347,\n",
       "  0.8810459971427917,\n",
       "  0.8659073710441589,\n",
       "  0.863847017288208,\n",
       "  0.7918704152107239,\n",
       "  0.82276451587677],\n",
       " 'eval_recall': [0.8572958707809448,\n",
       "  0.8365052938461304,\n",
       "  0.8531399369239807,\n",
       "  0.8740758299827576,\n",
       "  0.8064603805541992,\n",
       "  0.8349189758300781,\n",
       "  0.8539499044418335,\n",
       "  0.8083140850067139,\n",
       "  0.8339346647262573,\n",
       "  0.8334577679634094,\n",
       "  0.8492679595947266,\n",
       "  0.8399270176887512,\n",
       "  0.8211696147918701,\n",
       "  0.804031491279602,\n",
       "  0.8424698114395142,\n",
       "  0.8765357732772827,\n",
       "  0.8434001207351685,\n",
       "  0.8784790635108948,\n",
       "  0.8181613087654114,\n",
       "  0.8347580432891846,\n",
       "  0.8604185581207275,\n",
       "  0.8416140079498291,\n",
       "  0.8441694974899292,\n",
       "  0.8245598673820496,\n",
       "  0.8128722906112671,\n",
       "  0.8610548973083496,\n",
       "  0.8576056957244873,\n",
       "  0.8243431448936462,\n",
       "  0.8218053579330444,\n",
       "  0.8181592226028442,\n",
       "  0.8310396671295166,\n",
       "  0.8488695025444031,\n",
       "  0.8380000591278076,\n",
       "  0.8265727162361145,\n",
       "  0.8527402877807617,\n",
       "  0.8495803475379944,\n",
       "  0.8221045136451721,\n",
       "  0.8459295630455017,\n",
       "  0.8902339935302734,\n",
       "  0.8509817123413086,\n",
       "  0.8467609882354736,\n",
       "  0.8445807099342346,\n",
       "  0.796486496925354],\n",
       " 'eval_f1': [0.8635424375534058,\n",
       "  0.8399312496185303,\n",
       "  0.8735630512237549,\n",
       "  0.8820694088935852,\n",
       "  0.81415855884552,\n",
       "  0.8347808122634888,\n",
       "  0.8562762141227722,\n",
       "  0.8306884169578552,\n",
       "  0.8380915522575378,\n",
       "  0.8402053713798523,\n",
       "  0.8481671810150146,\n",
       "  0.8435354232788086,\n",
       "  0.827612042427063,\n",
       "  0.8120667338371277,\n",
       "  0.8419867157936096,\n",
       "  0.8807908892631531,\n",
       "  0.8412622809410095,\n",
       "  0.8805124759674072,\n",
       "  0.8256369829177856,\n",
       "  0.8384520411491394,\n",
       "  0.8543092012405396,\n",
       "  0.8340188264846802,\n",
       "  0.855270266532898,\n",
       "  0.8438761830329895,\n",
       "  0.8308687210083008,\n",
       "  0.867324709892273,\n",
       "  0.8405528664588928,\n",
       "  0.8313628435134888,\n",
       "  0.8334169983863831,\n",
       "  0.7671559453010559,\n",
       "  0.8486431837081909,\n",
       "  0.8421206474304199,\n",
       "  0.851223349571228,\n",
       "  0.8320125341415405,\n",
       "  0.8518021106719971,\n",
       "  0.8408094048500061,\n",
       "  0.8279206156730652,\n",
       "  0.8423696160316467,\n",
       "  0.8856161236763,\n",
       "  0.8583796620368958,\n",
       "  0.8552186489105225,\n",
       "  0.8173766732215881,\n",
       "  0.8094123005867004],\n",
       " 'eval_hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.35.0)',\n",
       " 'eval_runtime': 18.2684,\n",
       " 'eval_samples_per_second': 2.354,\n",
       " 'eval_steps_per_second': 0.328}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# def perform_summarization(sample):\n",
    "#     inputs = [prefix + doc for doc in sample[\"documents\"]]\n",
    "#     return summarizer(inputs, max_length=30)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': \"this is a psa, for all of you who think dancing with no dick restriction is a good idea. \\nso i was just finished banging my lovely lady friend. we haven't gotten to do it in a while as she was on a vacation with her family for a wedding, so when she got back after almost 2 weeks we had some fun. i was so enthusiastic and happy that after i was done, i got up out if bed and decided to use my still semi-erect penis as a tool of entertainment to express my joy. i helicoptered like i have never helicoptered before and as my girlfriend giggled i began to lose the erection, but i guess because if the motion, it stayed heavy and long enough to have some serious momentum. well she said something that made me lose rhythm and i swung down when i should've swung right and my meat club smashed into my low dangling fuzzy peaches, which were were swinging forward. i collapsed on the ground naked and held my funny bits cringing on the floor as my lady had the longest laugh of her life. \\nso yes, til you can hit yourself in the balls with your dick, and it fucking hurts.\",\n",
       " 'summary': 'i danced too hard and smashed my fun pouch with my disco stick.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "def prettyprint(text):\n",
    "    print(textwrap.fill(text, 150), \"\\n\")\n",
    "    \n",
    "res_dataset = concatenate_datasets([dataset[k] for k in dataset.column_names])\n",
    "for col in res_dataset.column_names:\n",
    "    if col in globals.new_col_names.keys():\n",
    "        res_dataset = res_dataset.rename_column(col, globals.new_col_names[col])\n",
    "res_dataset = res_dataset.select_columns(['document', 'summary']) \n",
    "\n",
    "all_data = res_dataset[0]\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a psa, for all of you who think dancing with no dick restriction is a good idea.  so i was just finished banging my lovely lady friend. we\n",
      "haven't gotten to do it in a while as she was on a vacation with her family for a wedding, so when she got back after almost 2 weeks we had some fun.\n",
      "i was so enthusiastic and happy that after i was done, i got up out if bed and decided to use my still semi-erect penis as a tool of entertainment to\n",
      "express my joy. i helicoptered like i have never helicoptered before and as my girlfriend giggled i began to lose the erection, but i guess because if\n",
      "the motion, it stayed heavy and long enough to have some serious momentum. well she said something that made me lose rhythm and i swung down when i\n",
      "should've swung right and my meat club smashed into my low dangling fuzzy peaches, which were were swinging forward. i collapsed on the ground naked\n",
      "and held my funny bits cringing on the floor as my lady had the longest laugh of her life.  so yes, til you can hit yourself in the balls with your\n",
      "dick, and it fucking hurts. \n",
      "\n",
      "i danced too hard and smashed my fun pouch with my disco stick. \n",
      "\n",
      "i was just finished banging my lovely lady friend . she was on a vacation with her family for a wedding . when she got back she decided to use her\n",
      "still semi-erect penis as a tool . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "t5_summarizer = pipeline(\"summarization\", model='t5-small')\n",
    "t5_summaries = t5_summarizer(\"summarize: \" + all_data['document'], min_length=30, max_length=globals.max_target_length['reddit'])\n",
    "t5_summaries\n",
    "\n",
    "prettyprint(all_data['document'])\n",
    "prettyprint(all_data['summary'])\n",
    "prettyprint(t5_summaries[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": " `args[0]`: Dataset({\n    features: ['document', 'summary'],\n    num_rows: 10\n}) have the wrong format. The should be either of type `str` or type `list`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sub_sample\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y104sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m t5_summarizer \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39msummarization\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y104sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m t5_summarizer(sub_sample, batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39monly_first\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(out)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y104sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# [{'label': 'POSITIVE', 'score': 0.9998743534088135}]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y104sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Exactly the same output as before, but the content are passed\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# as batches to the model\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:269\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[1;32m    170\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[1;32m    171\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[1;32m    172\u001b[0m     ):\n\u001b[1;32m    173\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/pipelines/base.py:1146\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpreprocess_params)\n\u001b[1;32m   1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:177\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.preprocess\u001b[0;34m(self, inputs, truncation, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(\u001b[39mself\u001b[39m, inputs, truncation\u001b[39m=\u001b[39mTruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 177\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_and_tokenize(inputs, truncation\u001b[39m=\u001b[39;49mtruncation, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    178\u001b[0m     \u001b[39mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py:129\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._parse_and_tokenize\u001b[0;34m(self, truncation, *args)\u001b[0m\n\u001b[1;32m    127\u001b[0m     padding \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    130\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `args[0]`: \u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m have the wrong format. The should be either of type `str` or type `list`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m     )\n\u001b[1;32m    132\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\u001b[39m*\u001b[39margs, padding\u001b[39m=\u001b[39mpadding, truncation\u001b[39m=\u001b[39mtruncation, return_tensors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework)\n\u001b[1;32m    133\u001b[0m \u001b[39m# This is produced by tokenizers but is an invalid generate kwargs\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m:  `args[0]`: Dataset({\n    features: ['document', 'summary'],\n    num_rows: 10\n}) have the wrong format. The should be either of type `str` or type `list`"
     ]
    }
   ],
   "source": [
    "sub_sample = res_dataset.select([i for i in range(10)])\n",
    "sub_sample\n",
    "t5_summarizer = pipeline(\"summarization\", model='t5-small')\n",
    "for out in t5_summarizer(sub_sample, batch_size=8, truncation=\"only_first\"):\n",
    "    print(out)\n",
    "    # [{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n",
    "    # Exactly the same output as before, but the content are passed\n",
    "    # as batches to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a psa, for all of you who think dancing with no dick restriction is a good idea.  so i was just finished banging my lovely lady friend. we\n",
      "haven't gotten to do it in a while as she was on a vacation with her family for a wedding, so when she got back after almost 2 weeks we had some fun.\n",
      "i was so enthusiastic and happy that after i was done, i got up out if bed and decided to use my still semi-erect penis as a tool of entertainment to\n",
      "express my joy. i helicoptered like i have never helicoptered before and as my girlfriend giggled i began to lose the erection, but i guess because if\n",
      "the motion, it stayed heavy and long enough to have some serious momentum. well she said something that made me lose rhythm and i swung down when i\n",
      "should've swung right and my meat club smashed into my low dangling fuzzy peaches, which were were swinging forward. i collapsed on the ground naked\n",
      "and held my funny bits cringing on the floor as my lady had the longest laugh of her life.  so yes, til you can hit yourself in the balls with your\n",
      "dick, and it fucking hurts. \n",
      "\n",
      "i danced too hard and smashed my fun pouch with my disco stick. \n",
      "\n",
      "summarize: this is a psa, for all of you who think dancing with no dick restriction is a good idea. _________________so i was just finished banging my\n",
      "lovely lady friend. we haven't gotten to do it in a while as she was on a vacation with her family for a wedding, so when she got back after almost 2\n",
      "weeks we had some fun. i was so enthusiastic and happy that after i was done, i got up out if bed and decided \n",
      "\n"
     ]
    }
   ],
   "source": [
    "bart_summarizer = pipeline(\"summarization\", model='facebook/bart-base')\n",
    "bart_summaries = bart_summarizer(\"summarize: \" + all_data['document'], min_length=30, max_length=globals.max_target_length['reddit'])\n",
    "bart_summaries\n",
    "\n",
    "prettyprint(all_data['document'])\n",
    "prettyprint(all_data['summary'])\n",
    "prettyprint(bart_summaries[0]['summary_text'])\n",
    "# prettyprint(baser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BartTokenizer, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = 'facebook/bart-base'\n",
    "# subject = 'reddit'\n",
    "\n",
    "# tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2632: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# input_tokenized = tokenizer.batch_encode_plus(all_data['document'], return_tensors='pt', max_length=1024, padding=True)\n",
    "# input_tokens = input_tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# print(input_tokenized.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     0,    90,  ...,     4,    90,     2],\n",
      "        [    2,     0,   118,  ...,     4,     2,     1],\n",
      "        [    2,     0,   118,  ...,     4,   118,     2],\n",
      "        ...,\n",
      "        [    2,     0,    90,  ...,     4,    90,     2],\n",
      "        [    2,     0,    29,  ..., 48071,     2,     1],\n",
      "        [    2,     0, 24970,  ..., 48071,     2,     1]])\n"
     ]
    }
   ],
   "source": [
    "# encoded_ids = model.generate(\n",
    "#     input_tokens,\n",
    "#     num_beams=4,\n",
    "#     length_penalty=2.0,\n",
    "#     max_length=globals.max_target_length[subject],\n",
    "#     min_length=10\n",
    "# )\n",
    "# print(encoded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "tensor([  2,   0,  90,   4, 438,   4, 428,   4,  90,   4,  90,   2])\n"
     ]
    }
   ],
   "source": [
    "# print(encoded_ids[0].shape)\n",
    "# print(encoded_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     0,    90,  ...,     4,    90,     2],\n",
       "        [    2,     0,   118,  ...,     4,     2,     1],\n",
       "        [    2,     0,   118,  ...,     4,   118,     2],\n",
       "        ...,\n",
       "        [    2,     0,    90,  ...,     4,    90,     2],\n",
       "        [    2,     0,    29,  ..., 48071,     2,     1],\n",
       "        [    2,     0, 24970,  ..., 48071,     2,     1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoded_ids.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/madisonthantu/Desktop/COMS 6998/Final Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m summary \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mdecode(encoded_ids\u001b[39m.\u001b[39;49msqueeze(), skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# print(textwrap.fill(summary, 100))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/madisonthantu/Desktop/COMS%206998/Final%20Project/recursive_LLMs/Preliminary_Exploration/reddit_dataset_exploration.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m summary\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3746\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3744\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3747\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3748\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3749\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3750\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3751\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[1;32m    992\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    993\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    998\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1001\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m   1002\u001b[0m     legacy_added_tokens \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_added_tokens_encoder\u001b[39m.\u001b[39mkeys()) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_tokens) \u001b[39m|\u001b[39m {\n\u001b[1;32m   1003\u001b[0m         token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madditional_special_tokens \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_tokens_to_ids(token) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m   1004\u001b[0m     }\n\u001b[1;32m   1005\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/COMS 6998/Final Project/recursive_LLMs/langgen/lib/python3.10/site-packages/transformers/tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    974\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    975\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[0;32m--> 976\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index)\n\u001b[1;32m    977\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[1;32m    978\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "# summary = tokenizer.decode(encoded_ids.squeeze(), skip_special_tokens=True)\n",
    "# # print(textwrap.fill(summary, 100))\n",
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
