{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-14.1-arm64-arm-64bit\n",
      "https://api-inference.huggingface.co/models/s-nlp/roberta-base-formality-ranker\n",
      "macOS-14.1-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"src/Measurement\"))\n",
    "import globals\n",
    "from src.Measurement.measurement import Measurement\n",
    "from src.utils import *\n",
    "globals.init()\n",
    "print(globals.API_URL)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from datasets import concatenate_datasets, load_dataset, DatasetDict, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "import platform\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, BartModel, GPT2Model, T5ForConditionalGeneration\n",
    "print(platform.platform())\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    login(token=globals.hug_token)\n",
    "    \n",
    "\n",
    "def make_output_base_path_str():\n",
    "    path = os.path.join(args['output_dir'], args['base_model'], args['dataset'])\n",
    "    print(f\"Output directory path: {path}\")\n",
    "    if os.path.isdir(path):\n",
    "        print(f\"The supplied output directory, {[path]}, already exists. Do you wish to overwrite this directory's contents? [y/n]: \")\n",
    "        if str(input()).lower() != \"y\":\n",
    "            sys.exit()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_function(examples, tokenizer, prefix = \"summarize: \"):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=globals.max_src_length[args['dataset']], truncation=False)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=globals.max_target_length[args['dataset']], truncation=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def load_huggingface_dataset(data_config):\n",
    "    data_dict = load_dataset(*data_config)\n",
    "    dataset = concatenate_datasets([data_dict[k] for k in data_dict.keys()])\n",
    "    # print(dataset.column_names)\n",
    "    for col in dataset.column_names:\n",
    "        if col in globals.new_col_names.keys():\n",
    "            dataset = dataset.rename_column(col, globals.new_col_names[col])\n",
    "    #         print(col, globals.new_col_names[col])\n",
    "    # print(dataset.column_names)\n",
    "    if args['dataset'] == 'reddit':\n",
    "        return dataset.select_columns(['document', 'summary'])\n",
    "    else:\n",
    "       return dataset.select_columns(['id', 'document', 'summary'])\n",
    "    \n",
    "    \n",
    "def train_val_test_split(dataset):\n",
    "    data_dict = dataset.train_test_split(test_size=0.3)\n",
    "    dev_data_dict = data_dict['test'].train_test_split(test_size=0.5)\n",
    "    # print()\n",
    "    # print(data_dict)\n",
    "    # print()\n",
    "    # print(dev_data_dict)\n",
    "    return DatasetDict({\n",
    "        'train': data_dict['train'],\n",
    "        'validation': dev_data_dict['train'],\n",
    "        'test': dev_data_dict['test']\n",
    "    })\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge_res = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    rouge_res[\"summary_length\"] = np.mean(prediction_lens)\n",
    "    results = {k: round(v, 4) for k, v in rouge_res.items()}\n",
    "    \n",
    "    bert_res = bert_score.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    results.update({k: round(v,4) for k, v in bert_res.items()})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(file_path, file_name):\n",
    "    with open(os.path.join(file_path , file_name), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    \n",
    "def load_synthetic_dataset(synthetic_data_path, file_name):\n",
    "    data_df = pd.read_pickle(os.path.join(synthetic_data_path, file_name))\n",
    "    dataset = Dataset.from_pandas(data_df)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_files_to_pkl(path_dict, parent_dir):\n",
    "    for path, values in path_dict.keys():\n",
    "        pickle.dump(values, open(os.path.join(parent_dir, path), \"wb\" ) )\n",
    "    print(\"Files saved ðŸ’ƒðŸ•º\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_summarization(sample):\n",
    "    return summarizer(sample['documents'], max_length=30)\n",
    "    \n",
    "    \n",
    "def generate_new_synthetic_dataset(generation_path):\n",
    "    summarizer = pipeline(\"summarization\", model=generation_path)\n",
    "    summaries = dataset.map(lambda data_x: perform_summarization(data_x), batched=True)\n",
    "    synthetic_df = pd.DataFrame({\n",
    "        'document': dataset['document'],\n",
    "        'summary': summaries\n",
    "        })\n",
    "    if 'id' in dataset.column_names:\n",
    "        synthetic_df['id'] = dataset['id']\n",
    "    return synthetic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='running parameter experiments')\n",
    "# parser.add_argument('--output_dir', type=str, default='test')\n",
    "# parser.add_argument('--base_model', type=str, default='t5')\n",
    "# parser.add_argument('--dataset', type=str, default='news')\n",
    "# parser.add_argument('--num_generations', type=int, default=3)\n",
    "\n",
    "parser = {\n",
    "    'output_dir':'test',\n",
    "    'base_model':'t5',\n",
    "    'dataset':'dialogue',\n",
    "    'num_generations':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/madisonthantu/.cache/huggingface/token\n",
      "Login successful\n",
      "Output directory path: test/t5/dialogue\n",
      "base_path = test/t5/dialogue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0\n",
      "['id', 'dialogue', 'summary']\n",
      "dialogue document\n",
      "['id', 'document', 'summary']\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'summary'],\n",
      "        num_rows: 11458\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'summary'],\n",
      "        num_rows: 4911\n",
      "    })\n",
      "})\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'summary'],\n",
      "        num_rows: 2455\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'summary'],\n",
      "        num_rows: 2456\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7151b6c24141529fd9f1d607658f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080890650fd6434b90b37862cb689516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d441aeebd2e4e8da92414e2daca1bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a6a42508974344bf13340cc29199f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7c65aae7764c32a0b4356817082d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c947e4491822482fa13a6345f46cff0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d019a5c8cff486f8a2c18224c4b9846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11338 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6529a74b026243f5b5d475ee4bbc48e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9a168b8ece4aba9a8424d260f667d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2425 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65186c4032dd4d1daaa26d9698d8b197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54309d3bbedf4af3b24016ab0f35402d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc26c61a079a499da62be508d2efc8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "What it do:\n",
    "    1. fine tune the model\n",
    "    2. compute_metrics results and save\n",
    "    2. save the new checkpoint\n",
    "    3. generate new synthetic dataset\n",
    "    4. perform measurement analysis on the new dataset\n",
    "\"\"\"\n",
    "init()\n",
    "# parser = argparse.ArgumentParser(description='running parameter experiments')\n",
    "# parser.add_argument('--output_dir', type=str, required=True)\n",
    "# parser.add_argument('--base_model', type=str, required=True)\n",
    "# parser.add_argument('--dataset', type=str, required=True)\n",
    "# parser.add_argument('--num_generations', type=int, default=3)\n",
    "\n",
    "# parsed_args = parser.parse_args()\n",
    "args = parser\n",
    "\n",
    "base_models = {\n",
    "    't5': 't5-small',\n",
    "    'bart': 'facebook/bart-base',\n",
    "    'gpt': 'gpt2'\n",
    "}\n",
    "ModelConstructor = {\n",
    "    't5':T5ForConditionalGeneration,\n",
    "    'bart': BartModel,\n",
    "    'gpt': GPT2Model\n",
    "}\n",
    "learning_rates = {\n",
    "    't5': 1e-4,\n",
    "    'bart': 2e-5,\n",
    "    'gpt2': 2e-5\n",
    "}\n",
    "dataset_configs = {\n",
    "    'news': [\"cnn_dailymail\", \"2.0.0\"],\n",
    "    'reddit': [\"reddit_tifu\", 'long'],\n",
    "    'dialogue': ['samsum']\n",
    "}\n",
    "\n",
    "assert(args['base_model'] in base_models.keys()), \"Invalid 'base_model' supplied\"\n",
    "assert(args['dataset'] in dataset_configs.keys()), \"Invalid 'dataset' suplied\"\n",
    "\n",
    "base_path = make_output_base_path_str()\n",
    "print(\"base_path =\", base_path)\n",
    "\n",
    "dataset_key, dataset_config = args['dataset'], dataset_configs[args['dataset']]\n",
    "base_model = args['base_model']\n",
    "base_model_checkpoint = base_models[base_model]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_checkpoint)\n",
    "\n",
    "for gen_num in tqdm(range(args['num_generations'])):\n",
    "    print(f\"Generation {gen_num}\")\n",
    "    generation_path = os.path.join(base_path, f\"generation{gen_num}\")\n",
    "    assert(not os.path.exists(generation_path))\n",
    "    os.makedirs(generation_path)\n",
    "    os.makedirs(os.path.join(generation_path, 'synthetic_data'))\n",
    "    \n",
    "    if gen_num == 0:\n",
    "        model_checkpoint = base_model_checkpoint\n",
    "        dataset = load_huggingface_dataset(dataset_config)\n",
    "        \n",
    "    else:\n",
    "        model_checkpoint = os.path.join(base_path, f\"generation{gen_num-1}\")\n",
    "        dataset = load_synthetic_dataset(os.path.join(model_checkpoint, \"synthetic_data\"), \"synthetic_data.pkl\")\n",
    "        \n",
    "        \n",
    "    data_dict = train_val_test_split(dataset)\n",
    "    \n",
    "    tokenized_data = data_dict.map(lambda data_x: preprocess_text_function(data_x, tokenizer), batched=True)\n",
    "    tokenized_data = tokenized_data.filter(lambda example: len(example['input_ids']) <= globals.max_src_length[args['dataset']])\n",
    "    tokenized_data = tokenized_data.filter(lambda example: len(example['labels']) <= globals.max_target_length[args['dataset']])\n",
    "    \n",
    "    assert(tokenized_data['train'] != tokenized_data['test'])\n",
    "    assert(tokenized_data['train'] != tokenized_data['validation'])\n",
    "    assert(tokenized_data['validation'] != tokenized_data['test'])\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_checkpoint)\n",
    "    \n",
    "    model = ModelConstructor[base_model].from_pretrained(model_checkpoint)\n",
    "    \n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    bert_score = evaluate.load(\"bertscore\")\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=generation_path,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=learning_rates[base_model],\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=1,\n",
    "        predict_with_generate=True,\n",
    "        push_to_hub=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # trainer.train()\n",
    "    trainer.save_model()\n",
    "    \n",
    "    test_data_eval_results = trainer.evaluate(tokenized_data[\"test\"])\n",
    "    print(test_data_eval_results)\n",
    "    \n",
    "    # Generate new synthetic dataset\n",
    "    new_dataset = generate_new_synthetic_dataset(generation_path)\n",
    "    dataset_specs = {\n",
    "        'generation':gen_num, \n",
    "        'subject':args['dataset']\n",
    "    }\n",
    "    synthetic_dataset_measurements = Measurement(new_dataset, dataset_specs, DEBUG=True)\n",
    "    synthetic_dataset_results = synthetic_dataset_measurements.measure()\n",
    "    \n",
    "    files_to_save = {\n",
    "        'test_data_eval_results.pkl': test_data_eval_results,\n",
    "        'synthetic_data/synthetic_data.pkl': new_dataset,\n",
    "        'synthetic_data/config.pkl': synthetic_dataset_results['config'],\n",
    "        'synthetic_data/measurements.pkl': synthetic_dataset_results['metrics']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
