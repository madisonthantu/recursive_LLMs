# recursive_LLMs
Repository for course project for COMS 6998 - Language Generation &amp; Summarization

### Abstract 
The data that is used to train large language models (LLMs) influences the decisions that it makes. Furthermore, the rate at which such models can produce content dramatically outweighs that which a human being can. The present research examines the effects of training LLMs on synthetic data, where that synthetic data was generated by a previous iteration of that model, and evaluates on the task of abstractive text summarization, although the synthetic data pipeline is relevant in nearly all of the applications for which these models are used. This research is intersted in both the quantitative and qualitative dimensions along which generated text evolves. Specifically, I consider the text characteristics of toxicity, formality, and emotion intensity, using three different abstractive summarization datasets, the set of which were selected with the intention of representing different text characteristics to differing degrees. Results support existing evidence that recursive training can reduce output quality.

The relevant scripts are:

[1] ```run_summarization.py```.
This script is adopted from [this Hugging face tutorial](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization). It is used for the summarization task with encoder-decoder models: used in the research of this repository are the the ```BartForConditionalGeneration``` and ```T5ForConditionalGeneration``` architectures.

An example command for running this script using the T5 model with the ```t5-base``` huggingface checkpoint is:

```
python run_summarization.py \
    --model_name_or_path t5-base \
    --do_train 1 \
    --do_eval 1 \
    --do_predict 1 \
    --data_path Data/initial_datasets/news \
    --num_epochs 1 \
    --output_dir results/t5/gen0 \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate 1 \
    --gen_num 0 \
    --dataset_name news \
    --base_model t5 \

```

[2] ```run_summarization_gpt2.py```.
This script is adopted from the same Hugginface tutorial as above; however, it is tailored to work with the GPT-2 model, which is a decoder only model. This present research uses ```GPT2LMHeadModel``` architecture.

An example command for running this script with the ```gpt2``` huggingface checkpoint is:
```
python run_summarization_gpt2.py \
    --model_name_or_path gpt2 \
    --do_train 1 \
    --do_eval 1 \
    --do_predict 1 \
    --data_path Data/initial_datasets/news \
    --num_epochs 1 \
    --output_dir results/gpt2/gen0 \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate 1 \
    --gen_num 0 \
    --dataset_name news \
    --base_model gpt2 \
```

[3] ```src/Measurement/run_measurement_analysis.py```.
This script computes the dataset metrics for a supplied dataset. Dataset metrics include: average_doc_tokens, average_summary_tokens, formality_scores, toxicity_scores, coverage, compression_ratio, etc.

Example for running this script:
```
python src/Measurement/run_measurement_analysis.py \
    --data_path ... \
    --dataset_name reddit \
    --generation 0 \
    --output_dir results \
    --model bart
```

[3] ```src/Measurement/run_measurement_analysis.py```.
This script computes the dataset metrics for a supplied dataset. Dataset metrics include: average_doc_tokens, average_summary_tokens, formality_scores, toxicity_scores, coverage, compression_ratio, etc.

Example for running this script:
```
python src/Measurement/run_measurement_analysis.py \
    --data_path ... \
    --dataset_name reddit \
    --generation 0 \
    --output_dir results \
    --model bart
```

**Data processing scripts:**

[4] ```Data/scripts/compute_metrics.py```.
This script computes rouge evaluation metrics between a set of synthetic data summaries and either real summaries or another set of synthetic summaries.

Example for running this script:
```
python compute_metrics.py \
    --synth_data_path synthetic_datasets/dialogue/bart/gen0 \
    --init_data_path initial_datasets/dialogue \
    --dataset_name dialogue \
    --base_model bart
```

[5] ```Data/scripts/initial_datasets_to_disk.py```.
This script downloads the datasets from Huggingface and saves them to disk.

Example for running this script:
```
python initial_datasets_to_disk.py \
    --dataset_name news
```

[6] ```Data/scripts/preprocess_dataset.py```.
This script preprocesses a given dataset, removing rows that are missing any values.

Example for running this script:
```
python preprocess_dataset.py \
    --data_dir synthetic_datasets/reddit/bart \
    --file_name full_data.csv \
    --dataset_name reddit
```